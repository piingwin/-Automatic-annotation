#!/usr/bin/env python
# -*- coding: utf-8 -*-

import urllib.request
import pymorphy2
import math
import xml.sax
import time
import sqlite3

def get_file_name(lang):
    return '%swiki-latest-pages-articles.xml' % lang

def load_dump():
    lang = input("Lang: ").lower()
    file = get_file_name(lang) + '.bz2'
    
    url = 'https://dumps.wikimedia.org/%swiki/latest/%s' % (lang, file)
    req = urllib.request.urlopen(url)
    
    total_size = req.headers.get('Content-Length').strip()
    total_size = int(total_size)
    
    downloaded = 0
    downloaded_total_two = 0
    
    CHUNK = 8192
    
    with open(file, 'wb+') as h:
        while True:
            chunk = req.read(CHUNK)
            downloaded += len(chunk)
            
            downloaded_total_one = math.floor(100 * downloaded / total_size)
            if downloaded_total_one != downloaded_total_two:
                downloaded_total_two = downloaded_total_one
                print('loaded already: ', downloaded_total_two, '%')
            
            if not chunk: 
                break
            
            h.write(chunk)

def find_articles(lang):
    def sortByAlphabet(inputStr):
        return inputStr[0].lower()
  
    class FindTitleHandler(xml.sax.ContentHandler):
        def __init__(self):
            self.morph = pymorphy2.MorphAnalyzer()
            self.CurrentData = ""
            self.title = ""
            self.content = ""
            
            self.con = sqlite3.connect('articles.db')
            self.cur = self.con.cursor()
            self.cur.execute("""CREATE TABLE IF NOT EXISTS articles(
                                id serial PRIMARY KEY,
                                article VARCHAR(200) unique not null,
                                links integer,
                                usage integer,
                                status boolean default true
                                )
                            """)
            
            self.cur.execute('CREATE INDEX IF NOT EXISTS iarticle ON articles (article)')
            
            #reset status of all articles to false
            self.cur.execute('update articles set status = 0')
            self.con.commit()

        def forms(self):
            match = self.morph.parse(self.title)[0]
            return match.lexeme
        
        # Call when an element starts
        def startElement(self, tag, attributes):
            self.CurrentData = tag
        
        # Call when an elements ends
        def endElement(self, tag):
            if self.CurrentData == "title":
                self.title = self.content
                print(self.title)
            if self.CurrentData == "text":
                links = self.content.count('[[')
                forms = self.forms()
                usage = 0
                for i in forms:
                    usage += self.content.count(i.word.lower())
                
                # pull all in our database
                self.cur.execute("""insert or replace into articles (id, article, links, usage, status) 
                                  values ((select id from articles where article = '%s'), '%s', %d, %d, 1)""" % (self.title, self.title, links, usage))
                self.con.commit()
                
        # Call when a character is read
        def characters(self, content):
            if self.CurrentData == "title":
                self.content = content
            if self.CurrentData == "text":
                self.content = self.content + content.lower()

        def __del__(self):
            self.cur.execute("delete from articles where status = 0") 
            self.con.commit()
            self.con.close()
        
    file = get_file_name(lang)
    #file = 'test3.xml'   

    parser = xml.sax.make_parser()
    # turn off namepsaces
    parser.setFeature(xml.sax.handler.feature_namespaces, 0)
    # override the default ContextHandler
    Handler = FindTitleHandler()
    parser.setContentHandler(Handler)
    parser.parse(file)
    

if __name__ == '__main__':
    start = time.time()
    print(start)
    find_articles('ru')
    print(time.time() - start)
